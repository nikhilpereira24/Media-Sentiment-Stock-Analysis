{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Effect of News Media and White House Sentiment on the Stock Market\n",
    "\n",
    "#### Group 22 Julia Hoffman, Nikhil Pereira, Ryan Weiss\n",
    "hjulia99@vt.edu, nikhil24@vt.edu, ryanw99@vt.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualititative Section Step 1\n",
    "\n",
    "#### -Questions, problem, hypothesis, claim, context, motivation\n",
    "#### -Definitions, data, methods to be used\n",
    "#### -Rationale, assumptions, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Define the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandemic has caused great amount of stress on the US Economy. People have lost jobs, closed businesses and sacrificed greatly to survive. Tension over the news, politics and direction of the country is increasing. The gatekeepers of information should be selective in what they say to the public, because their words can have a significant impact on the economy and peoples lives. Fluctuations in the stock market are common after certain articles and tweets reach Wall Street and the general public. While key information is sometimes the cause of these changes in the market, it's often the sentiment that has the most profound impact, and this can be carefully crafted through specific language. With social media, including Twitter, having grown tremendously over the past decade, the effect of tweets on the direction of the market has simultaenously grown, and now is a crucial time to study these effects.\n",
    "\n",
    "Additionally, with the COVID-19 situation being so new, there is not much research on how different sentiment has affected the stock market during this pandemic. Looking at those tweets have the largest effect on\n",
    "indexes like the DOW Jones Index and the S&P 500 is important in determining economic influence. This involves not only individuals like the president, but news organizations like Fox News, CNN, and MSCNB. Further, an analysis of these tweets and headlines can help influential people and organizations determine what language tends to have positive or negative effects on the market, providing useful guidance for future crises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will analyze the relationship between how the White House and the media portray COVID-19 updates and the stock market. Because these sources are the gatekeepers of information to the American people, they have a significant impact on the economy. Currently, Twitter is the #1 news app on the app store, suggesting that this cite is where many citizens obtain their information. Tweets will be gathered and analyzed from Donald Trump, CNN, FoxNews, and MSNBC for sentiment. In addition, data from the DOW Jones Index and the S&P 500 will be used to track the stock market. This is a significant project because if successful, it can inform the American public about how negative or positive headlines could be directly effecting their livelihood. It will also suggest to gatekeepers to think twice about their sentiment before publishing a Tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driving Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the effect of the White House and media sentiment on the stock market? \n",
    "#### Which gatekeeper has the larger effect on the stock market (S&P 500 and DOW Jones)? \n",
    "#### How should the American public and investors respond to news from each of the gatekeepers in the future?\n",
    "#### Is one gatekeeper causing more damage to the US Economy than others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Section Steps 2-5\n",
    "\n",
    "#### Data processing, analysis, visualization\n",
    "#### Documented code and results\n",
    "#### Summary visuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Collecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed Description of Data\n",
    "\n",
    "Data is collected from yahoo finance and Twitter to later be analyzed for a relationship between news media/White House sentiment and the stock market. The python module yfinance is used to obtain the opening and closing price of the DOW Jones Index and the S&P 500 Index. We plan to focus our analysis on 10 days prior to the crash and 10 days post crash in March 2020.The DOW and S&P charts show evidence that there was a steep crash in March 2020. As for the Twitter data, tweepy is used to access tweets from Trump, CNN, Fox News, and MSNBC. Six csvs are going to be generated, two for financial data and four for each Twitter user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install yahoofinancials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotated Code for Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Annotated Code for Financial Data\n",
    "\n",
    "from pandas_datareader import data\n",
    "# Getting Dow Jones and S&P 500 data\n",
    "dow = data.DataReader('DJIA', start = '2019-11-17', end='2020-05-05', data_source='yahoo')\n",
    "sp = data.DataReader('^GSPC', start = '2019-11-17', end='2020-05-05', data_source='yahoo')\n",
    "dow['Date']=dow.index\n",
    "sp['Date']=sp.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set our Auth Keys for Tweepy API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing API to access Twitter data \n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import json\n",
    "import pandas\n",
    "\n",
    "# Function to read the key file and load keys in a dictionary\n",
    "def loadKeys(key_file):\n",
    "    with open('keys.json') as f:\n",
    "        key_dict = json.load(f)\n",
    "    return key_dict['api_key'], key_dict['api_secret'], key_dict['token'], key_dict['token_secret']\n",
    "\n",
    "# Authenticate using oAuthHandler\n",
    "KEY_FILE = 'keys2.json'\n",
    "api_key, api_secret, token, token_secret = loadKeys(KEY_FILE)\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(token, token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Get User Data from All Twitter Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict #Import a dictionary data structure to parse the twitter object Record Set\n",
    "\n",
    "def get_user_data(username, numiterations,countTweet): #Function takes in the user number of tweets to \n",
    "    dataframe = pandas.DataFrame() #Create an empty dataframe\n",
    "    user_tweets = api.user_timeline(username, count = countTweet, exclude_replies = True, include_rts = False) #Call the Twttier API\n",
    "    user_dict = defaultdict(list) #Create an empty dictionary\n",
    "    lastTweet = user_tweets[-1] #Retrieve the last ID in the Record Set\n",
    "    newID = lastTweet.id #Store the last tweets ID\n",
    "    for i in range(numiterations): #Iterate n times to gather previous timeline data of user\n",
    "        tweets = api.user_timeline(username, count = countTweet, max_id=newID, exclude_replies = True, include_rts = False) #Call API with the last ID from previous timeline\n",
    "        for tweet in tweets: #Store the data from the API in a dictionary\n",
    "            user_dict[\"Date\"].append(tweet.created_at)\n",
    "            user_dict[\"text\"].append(tweet.text)\n",
    "            user_dict[\"ID\"].append(tweet.id)\n",
    "            user_dict[\"Username\"].append(username)\n",
    "        userdf = pd.DataFrame(user_dict) #Convert the dictionary to a dataframe\n",
    "        frame = pandas.concat([dataframe,userdf]) #Make a frame that concats the first set of data to the current frame userdf\n",
    "        newID = str(userdf.iloc[-1, :]['ID']) #Update the newID variable\n",
    "    return frame #return the frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Function with the Twitter User News Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FoxFrame = get_user_data('FoxNews',2,140).drop_duplicates() #Retreiving FoxNews Data\n",
    "pandas.set_option('display.max_rows', None)\n",
    "#FoxFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNNFrame = get_user_data('CNN',30,200).drop_duplicates() #Retreiving CNN News Data\n",
    "#CNNFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSNBCFrame = get_user_data('MSNBC',80,100).drop_duplicates().fillna(\"empty\") #Retreiving MSNBC Data\n",
    "#MSNBCFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrumpFrame = get_user_data('realDonaldTrump',10,1000).drop_duplicates() #Retreiving Trump Data\n",
    "#TrumpFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Data Collection Step 2\n",
    "\n",
    "The Data Collection step was mostly successful. As a team we were able to get the Financial Data quickly and easily. For the Twitter data, we devised a useful method that would iterate through the user's timeline to retrieve as many posts as possible. The only drawback we had to the Twitter data was that many of the tweets containt URLs rather than actual text. The data will need to be cleaned in the next steps in order to continue the Data Process Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save All Frames to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FoxFrame.to_csv('ProjectData/FoxData.csv')\n",
    "CNNFrame.to_csv('ProjectData/CnnData.csv')\n",
    "MSNBCFrame.to_csv('ProjectData/MSNBCData.csv')\n",
    "TrumpFrame.to_csv('ProjectData/TrumpTwitterData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Sentiment Analysis on Trump Data to show project path\n",
    "\n",
    "This is a sample 20 recent Tweets from Trump and we did a quick VADER sentiment analysis to show the direction of the project. This is a quick demonstration on how we will be doing Sentiment analysis on Twitter User Data before and after the stock market crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Score    0.02065\n",
      "Neutral Score     0.83520\n",
      "Positive Score    0.14415\n",
      "Compound Score    0.23309\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Negative Score</th>\n",
       "      <th>Neutral Score</th>\n",
       "      <th>Positive Score</th>\n",
       "      <th>Compound Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-28 20:49:37</td>\n",
       "      <td>THANK YOU ARIZONA! #MAGA \\nhttps://t.co/QgUI58...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.5461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-28 18:54:01</td>\n",
       "      <td>Why isn’t Twitter trending Biden corruption? I...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.819</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-28 16:36:55</td>\n",
       "      <td>Media and Big Tech are not covering Biden Corr...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-28 16:33:31</td>\n",
       "      <td>The USA doesn’t have Freedom of the Press, we ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.2732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-28 16:27:33</td>\n",
       "      <td>https://t.co/YGAn4QULA4</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-10-28 15:22:16</td>\n",
       "      <td>It’s amazing. Twitter refuses to allow the any...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.8147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-10-28 15:20:29</td>\n",
       "      <td>Volunteer to be a Trump Election Poll Watcher....</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-10-28 14:51:48</td>\n",
       "      <td>Incredible evening, incredible people. Love Ne...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.8711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-10-28 12:35:47</td>\n",
       "      <td>Covid, Covid, Covid is the unified chant of th...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-10-28 12:15:44</td>\n",
       "      <td>Thank you Charles. She will be a great one! ht...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.7840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-10-28 11:32:35</td>\n",
       "      <td>As a developer long ago, and continuing to thi...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-10-28 04:56:33</td>\n",
       "      <td>https://t.co/f1mqYvg6Fm</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-10-28 03:31:45</td>\n",
       "      <td>We are spending more in Florida, and we are wi...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.7783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-10-28 03:02:37</td>\n",
       "      <td>https://t.co/YGnjje9G4P</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-10-28 02:53:38</td>\n",
       "      <td>https://t.co/gsFSgh2KPc https://t.co/IriTZzvGtY</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-10-28 02:52:29</td>\n",
       "      <td>https://t.co/gsFSgh2KPc https://t.co/HoLVYSOU2I</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-10-28 02:28:41</td>\n",
       "      <td>THANK YOU NEBRASKA! Get your friends, get your...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.7856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-10-28 02:08:26</td>\n",
       "      <td>https://t.co/rEThigvI5W</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-10-28 02:08:16</td>\n",
       "      <td>https://t.co/FU5IncJ4VI</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-10-28 02:08:10</td>\n",
       "      <td>THANK YOU NEBRASKA! \\nhttps://t.co/k9DWvZO10b</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.5461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date                                               text  \\\n",
       "0   2020-10-28 20:49:37  THANK YOU ARIZONA! #MAGA \\nhttps://t.co/QgUI58...   \n",
       "1   2020-10-28 18:54:01  Why isn’t Twitter trending Biden corruption? I...   \n",
       "2   2020-10-28 16:36:55  Media and Big Tech are not covering Biden Corr...   \n",
       "3   2020-10-28 16:33:31  The USA doesn’t have Freedom of the Press, we ...   \n",
       "4   2020-10-28 16:27:33                            https://t.co/YGAn4QULA4   \n",
       "5   2020-10-28 15:22:16  It’s amazing. Twitter refuses to allow the any...   \n",
       "6   2020-10-28 15:20:29  Volunteer to be a Trump Election Poll Watcher....   \n",
       "7   2020-10-28 14:51:48  Incredible evening, incredible people. Love Ne...   \n",
       "8   2020-10-28 12:35:47  Covid, Covid, Covid is the unified chant of th...   \n",
       "9   2020-10-28 12:15:44  Thank you Charles. She will be a great one! ht...   \n",
       "10  2020-10-28 11:32:35  As a developer long ago, and continuing to thi...   \n",
       "11  2020-10-28 04:56:33                            https://t.co/f1mqYvg6Fm   \n",
       "12  2020-10-28 03:31:45  We are spending more in Florida, and we are wi...   \n",
       "13  2020-10-28 03:02:37                            https://t.co/YGnjje9G4P   \n",
       "14  2020-10-28 02:53:38    https://t.co/gsFSgh2KPc https://t.co/IriTZzvGtY   \n",
       "15  2020-10-28 02:52:29    https://t.co/gsFSgh2KPc https://t.co/HoLVYSOU2I   \n",
       "16  2020-10-28 02:28:41  THANK YOU NEBRASKA! Get your friends, get your...   \n",
       "17  2020-10-28 02:08:26                            https://t.co/rEThigvI5W   \n",
       "18  2020-10-28 02:08:16                            https://t.co/FU5IncJ4VI   \n",
       "19  2020-10-28 02:08:10      THANK YOU NEBRASKA! \\nhttps://t.co/k9DWvZO10b   \n",
       "\n",
       "           Username  Negative Score  Neutral Score  Positive Score  \\\n",
       "0   realDonaldTrump           0.000          0.532           0.468   \n",
       "1   realDonaldTrump           0.181          0.819           0.000   \n",
       "2   realDonaldTrump           0.000          1.000           0.000   \n",
       "3   realDonaldTrump           0.106          0.751           0.143   \n",
       "4   realDonaldTrump           0.000          1.000           0.000   \n",
       "5   realDonaldTrump           0.000          0.681           0.319   \n",
       "6   realDonaldTrump           0.000          1.000           0.000   \n",
       "7   realDonaldTrump           0.000          0.657           0.343   \n",
       "8   realDonaldTrump           0.126          0.769           0.105   \n",
       "9   realDonaldTrump           0.000          0.537           0.463   \n",
       "10  realDonaldTrump           0.000          1.000           0.000   \n",
       "11  realDonaldTrump           0.000          1.000           0.000   \n",
       "12  realDonaldTrump           0.000          0.755           0.245   \n",
       "13  realDonaldTrump           0.000          1.000           0.000   \n",
       "14  realDonaldTrump           0.000          1.000           0.000   \n",
       "15  realDonaldTrump           0.000          1.000           0.000   \n",
       "16  realDonaldTrump           0.000          0.743           0.257   \n",
       "17  realDonaldTrump           0.000          1.000           0.000   \n",
       "18  realDonaldTrump           0.000          1.000           0.000   \n",
       "19  realDonaldTrump           0.000          0.460           0.540   \n",
       "\n",
       "    Compound Score  \n",
       "0           0.5461  \n",
       "1          -0.6093  \n",
       "2           0.0000  \n",
       "3           0.2732  \n",
       "4           0.0000  \n",
       "5           0.8147  \n",
       "6           0.0000  \n",
       "7           0.8711  \n",
       "8          -0.1280  \n",
       "9           0.7840  \n",
       "10          0.0000  \n",
       "11          0.0000  \n",
       "12          0.7783  \n",
       "13          0.0000  \n",
       "14          0.0000  \n",
       "15          0.0000  \n",
       "16          0.7856  \n",
       "17          0.0000  \n",
       "18          0.0000  \n",
       "19          0.5461  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "#read the file into dataframe. \"header=0\" means the first row will be considered as a header\n",
    "##****** Make a dictionary and determine the data types correctly\n",
    "frame = pd.read_csv(\"ProjectData/TrumpTwitterData.csv\", header=0, dtype={'date':str,\"text\":str,'id':str, 'Username':str})\n",
    "analyzer = SentimentIntensityAnalyzer() #Store the sentiment variable\n",
    "sentimentDict = defaultdict(list)\n",
    "headframe = frame.head(20)\n",
    "for sentence in headframe.text: #Compute the scores for each sentence in frame\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    sentimentDict[\"Negative Score\"].append(vs['neg'])\n",
    "    sentimentDict[\"Neutral Score\"].append(vs['neu'])\n",
    "    sentimentDict[\"Positive Score\"].append(vs['pos'])\n",
    "    sentimentDict[\"Compound Score\"].append(vs['compound'])\n",
    "frame2 = pd.DataFrame(sentimentDict)\n",
    "userData = pd.concat([frame, frame2], axis=1, sort=False).drop(['Unnamed: 0','ID'],axis=1).iloc[0:20,:] #Concat the numeric and non-numeric data into one frame\n",
    "score = userData.mean()\n",
    "print(score)\n",
    "userData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Process Data\n",
    "\n",
    "In this step of the data process pipeline we need to clean the URL type tweets and find the text. To do this we will perform a webscraping method that using BeautifulSoup and requests. We will be creating functions to convert the URLs into text that way we can analyze the type of links the different Users are sharing. Another cleaning step that we have to do is to reformat the Date Time Columns so that they are easier to read when we graph the data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the URL Tweets from Phase 1: Gathering Twitter Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports vader sentiment analyzer, pandas, beautiful soup, and numpy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Importing tweet csvs as dataframes\n",
    "trumpTweets = pd.read_csv('ProjectData/TrumpTwitterData.csv').drop('Unnamed: 0', axis=1)\n",
    "cnnTweets = pd.read_csv('ProjectData/CnnData.csv').drop('Unnamed: 0', axis=1)\n",
    "foxTweets = pd.read_csv('ProjectData/FoxData.csv').drop('Unnamed: 0', axis=1)\n",
    "msnbcTweets = pd.read_csv('ProjectData/MSNBCData.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to pull title of page from url \n",
    "def get_title(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    return soup.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to map the correct title to each tweet \n",
    "def title_cleaner(text):\n",
    "    if 'https' in text[:5]:\n",
    "        return(get_title(str(text)))\n",
    "    else:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-447575621e10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfoxTweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfoxTweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle_cleaner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcnnTweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnnTweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle_cleaner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmsnbcTweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsnbcTweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle_cleaner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtrumpTweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrumpTweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle_cleaner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3380\u001b[0m         \"\"\"\n\u001b[0;32m   3381\u001b[0m         new_values = super(Series, self)._map_values(\n\u001b[1;32m-> 3382\u001b[1;33m             arg, na_action=na_action)\n\u001b[0m\u001b[0;32m   3383\u001b[0m         return self._constructor(new_values,\n\u001b[0;32m   3384\u001b[0m                                  index=self.index).__finalize__(self)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1218\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-22c62ddfcef0>\u001b[0m in \u001b[0;36mtitle_cleaner\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtitle_cleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'https'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-e23497dd466d>\u001b[0m in \u001b[0;36mget_title\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# Converting urls into headline titles\n",
    "foxTweets.text = foxTweets.text.map(title_cleaner)\n",
    "cnnTweets.text = cnnTweets.text.map(title_cleaner)\n",
    "msnbcTweets.text = msnbcTweets.text.map(title_cleaner)\n",
    "trumpTweets.text = trumpTweets.text.map(title_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-formatting the Date Time Stamp using Time Series Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function to map the reformatted date to the date columns\n",
    "def timeCleaner(date):\n",
    "    string1 = pandas.to_datetime(date)\n",
    "    string1 = string1.strftime('%B-%d-%Y')\n",
    "    return string1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Mapping the Date Column to that function\n",
    "foxTweets.Date = foxTweets.Date.map(timeCleaner)\n",
    "cnnTweets.Date = cnnTweets.Date.map(timeCleaner)\n",
    "msnbcTweets.Date = msnbcTweets.Date.map(timeCleaner)\n",
    "trumpTweets.Date = trumpTweets.Date.map(timeCleaner)\n",
    "dow.Date=dow.Date.map(timeCleaner)\n",
    "sp.Date=sp.Date.map(timeCleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Data Processing Step 3\n",
    "In this step we were able to clean the data by converting URLs into actual text and the date time object into a readable format. This cleaning step was crucial before moving on the future steps to visualize and analyze the data. Overall this step was fairly simple and only required a few functions because the API we used were good at returning clean data to begin with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Visualize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Financial Data of the DOW and SAP from the Past 4 Months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to focus our analysis on the last few months post crash in March 2020.The DOW and S&P charts show evidence that there was a steep crash in March 2020. As for the Twitter data, tweepy is used to access tweets from Trump, CNN, Fox News, and MSNBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()\n",
    "\n",
    "# # Making graph header \n",
    "f, (dowplot, spplot) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "f.suptitle('Dow Jones and S&P 500 Data Closing Prices Covid-19', fontsize=22)\n",
    "\n",
    "# Plotting the closing prices \n",
    "dow.Close.plot(title = 'DOW Jones Index', ax = dowplot, fontsize=12)\n",
    "dowplot.set_ylabel('Price')\n",
    "sp.Close.plot(title = 'S&P 500 Index', ax = spplot, fontsize=12)\n",
    "spplot.set_ylabel('Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two line plots show the trend of the DOW Jones and S&P 500 over the past 5 months or time that COVID-19 became apparent. As one can see these two indexes were at their peak for December 2019 - February 2020 before COVID 19 began to have an effect on the market. The volatility and panic hit the United States in late February and this corresponds to the time the government imposed the national lockdown and news outlets were covering the pandemic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Trump Data through sample Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userData.Date = userData.Date.map(timeCleaner)\n",
    "p = userData.plot.scatter(x='Negative Score', y='Positive Score',\n",
    "                   s =100, #marker size to population but divide to make smaller and add constant\n",
    "                   figsize=(8,8), #Make the figure size big enough\n",
    "                   alpha=0.8, #between 0 (transparent) and 1 (opaque). \n",
    "                   sharex=False) # sharex = convinces xlabel to show\n",
    "p.set_title('Trumps Tweet Sentiment of 20 Posts',fontsize=26) #Put a big title\n",
    "p.set_xlabel(\"Negative Score\", fontsize=18) #Put a x label\n",
    "p.set_ylabel(\"Positive Score\", fontsize=18) #Put a y label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trump's 20 Recent Post Sentiment\n",
    "\n",
    "In this scatter plot we can see a quick visual on how Trump's last 20 posts have been more positive leaning than negative. This sort of preliminary conclusion shows that Trump has been optimistic amid the Pandemic (End of April) and that we can probably infer the same conclusion when looking at end of March data. This is a sample sentiment visual that can be done for all the Twitter users and to analyze how their Twitter behaviour has been fluctuating through the COVID Pandemic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Analyze the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to use VADER sentiment analysis to examine the behaviour of the four Twitter users: MSNBC, CNN, FOX and realDonaldTrump. We want to understand their sentiment over the past few months so that we can determine which GateKeeper is having the greatest influence on the Stock Market. We are going to create a vader function to retrieve the scores and then combine all the data into one frame for analysis. Finally we will use a Bar Plot and Box Plot to visualize the summary statistics from the VADER sentiment analyzer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Sentiment on Clean  Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter is a csv file that's been read in using pandas, and has the tweet text in\n",
    "# a column titled 'text'.\n",
    "def vader_score_getter(data):\n",
    "\n",
    "    # Creates empty lists for the VADER scores\n",
    "    neg = []\n",
    "    neu = []\n",
    "    pos = []\n",
    "    compound = []\n",
    "    \n",
    "    # Gets the VADER scores for each tweet and adds them to the respective lists.\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for line in data['text']:\n",
    "        vs = analyzer.polarity_scores(line)\n",
    "        neg.append(vs['neg'])\n",
    "        neu.append(vs['neu'])\n",
    "        pos.append(vs['pos'])\n",
    "        compound.append(vs['compound'])\n",
    "    \n",
    "    # Adds the lists as columns to the dataframe\n",
    "    data['VADER negative score'] = neg\n",
    "    data['VADER neutral score'] = neu\n",
    "    data['VADER positive score'] = pos\n",
    "    data['VADER compound score'] = compound\n",
    "    \n",
    "    # Returns the new dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Vader Score Getter for each Twitter User (Fox, CNN, MSNBC, Trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe with positive, negative, and neutral sentiment of headline\n",
    "foxData2 = vader_score_getter(foxTweets)\n",
    "cnnData2 = vader_score_getter(cnnTweets)\n",
    "msnbcData2 = vader_score_getter(msnbcTweets)\n",
    "trumpData2 = vader_score_getter(trumpTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all the sentiment and frames into one Data Frame for Plotting and Comparisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.concat([foxData2, cnnData2, msnbcData2, trumpData2]).reset_index().iloc[:, 1:]\n",
    "#alldata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Analyzing the Sentiment for the Twitter Users through Bar Chart and Box Plot\n",
    "In this section of the data science experiment we are trying to visualize the VADER scores gathered in the alldata frame. By using the bar and box plots we can get visuals on how the sentiment of different Twitter users differ amid the Pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallel plot to see the scores grouped by each subreddit\n",
    "framePP = alldata.drop(['text','ID','Date','VADER neutral score'], axis=1).groupby('Username').mean()*100\n",
    "plot = framePP.plot.bar(figsize=(20, 6))\n",
    "plot.legend(loc=2, prop={'size': 12}) #Place a legend on the plot\n",
    "for tick in plot.xaxis.get_major_ticks(): #Adjust the x axis labels by adjusting their size and orientation\n",
    "                tick.label.set_fontsize(26) \n",
    "                tick.label.set_rotation('horizontal')\n",
    "for tick in plot.yaxis.get_major_ticks(): #Adjust the y axis labels by adjusting their size\n",
    "                tick.label.set_fontsize(16) \n",
    "plot.set_title('Sentiment of Tweets per Gatekeepers',fontsize=26) #Put a big title\n",
    "plot.set_xlabel(\"Twitter Users\", fontsize=18) #Put a x label\n",
    "plot.set_ylabel(\"Sentiment Percentage of Tweet\", fontsize=18) #Put a y label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making box plot header \n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "f.suptitle('Twitter - Positive Sentiment - Negative Sentiment', fontsize=14)\n",
    "\n",
    "# Plotting positive sentiment scores per gatekeeper\n",
    "seaborn.boxplot(x=\"Username\", y=\"VADER positive score\", data=alldata, ax=ax1)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation='vertical')\n",
    "ax1.set_xlabel(\"Gatekeeper\",size = 12,alpha=0.8)\n",
    "ax1.set_ylabel(\"Positive Sentiment Score\",size = 12,alpha=0.8)\n",
    "\n",
    "# Plotting negative sentiment scores per gatekeeper\n",
    "seaborn.boxplot(x=\"Username\", y=\"VADER negative score\", data=alldata, ax=ax2)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation='vertical')\n",
    "ax2.set_xlabel(\"Gatekeeper\",size = 12,alpha=0.8)\n",
    "ax2.set_ylabel(\"Negative Sentiment Score\",size = 12,alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Sentiment Graphs (Bar and Box Plots)\n",
    "The bar plot above shows how the positive, negative and compound score compare for each Twitter User. From the graphs we can see that Donald Trump's Twitter has the most positive sentiment and compound score (single normalized score) whereas FoxNews has the most negative sentiment and compound score. We can also see that CNN and MSNBC are mostly neutral because their compound score hovers around 0. This graph give us a good insight on how these Twitter Users behaved over the last few months with the COVID-19 Pandemic. Note that VADER mostly categorized everyone's tweets as neutral but we are comparing just the postive and negative metrics to make better conclusions.\n",
    "\n",
    "This box plot shows the variance of positive and negative sentiment between Tweets of each gatekeeper. Trump's tweets have the largest distribution of positive sentiment and the smallest distribution of negative sentiment with a few outliers. All three news outlets have relatively equal distributions for positive and negative sentiment suggesting that they are more neutral. Of the gatekeepers, FoxNews has the largest distribution of negative sentiment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeline of Gatekeeper Sentiment through Stock Market Crash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to analyze the sentiment of each gatekeeper around a few months after the stock market crash by looking at how the sentiment of the gatekeepers change overtime and comparing the results to the trends in the stock indicies we can determine if there is any patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Average Compound Score Per Day for Each Gatekeeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting sentiment over time to compare to stock market\n",
    "# Averaging the VADER scores of tweets from the same day to get a single data point for said day\n",
    "def average_data(dataframe):\n",
    "    dates = []\n",
    "    scores = []\n",
    "    dictionary = {'Date' : dates, 'VADER compound score' : scores}\n",
    "    for date in dataframe['Date']:\n",
    "        splitDF = dataframe[dataframe['Date'] == date]\n",
    "        scores.append(splitDF.mean()['VADER compound score'])\n",
    "        dates.append(date)\n",
    "    averagedDataFrame = pd.DataFrame(dictionary)\n",
    "    averagedDataFrame.drop_duplicates(subset=['Date'], keep=\"first\", inplace=True)\n",
    "    return averagedDataFrame\n",
    "\n",
    "foxDataAvg = average_data(foxData2).set_index('Date')\n",
    "cnnDataAvg = average_data(cnnData2).set_index('Date')\n",
    "msnbcDataAvg = average_data(msnbcData2).set_index('Date')\n",
    "trumpDataAvg = average_data(trumpData2).set_index('Date')\n",
    "\n",
    "# Reversing dataframe so the dates are in order\n",
    "foxDataRev = foxDataAvg[::-1]\n",
    "cnnDataRev = cnnDataAvg[::-1]\n",
    "msnbcDataRev = msnbcDataAvg[::-1]\n",
    "trumpDataRev = trumpDataAvg[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Average Compound Score versus Time and the Stock Market Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()\n",
    "\n",
    "\n",
    "# Making graph header\n",
    "f, (foxplot, cnnplot, dowplot) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "g, (msnbcplot, trumpplot, spplot) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "f.suptitle('Gatekeeper Sentiment Over Time vs stock Market Performance', fontsize=22)\n",
    "\n",
    "#Rotating for readability\n",
    "def rotate_ticks(plot):\n",
    "    for tick in plot.get_xticklabels():\n",
    "        tick.set_rotation('vertical')\n",
    "        \n",
    "\n",
    "# Plotting the compound score vs the date\n",
    "dow.Close.plot(title = 'DOW Jones Index', ax = dowplot, fontsize=12)\n",
    "rotate_ticks(dowplot)\n",
    "sp.Close.plot(title = 'S&P 500 Index', ax = spplot, fontsize=12)\n",
    "rotate_ticks(spplot)\n",
    "foxDataRev['VADER compound score'].plot(title = 'Fox', ax = foxplot)\n",
    "rotate_ticks(foxplot)\n",
    "cnnDataRev['VADER compound score'].plot(title = 'CNN', ax = cnnplot)\n",
    "rotate_ticks(cnnplot)\n",
    "msnbcDataRev['VADER compound score'].plot(title = 'MSNBC', ax = msnbcplot)\n",
    "rotate_ticks(msnbcplot)\n",
    "trumpDataRev['VADER compound score'].plot(title = 'Trump', ax = trumpplot)\n",
    "rotate_ticks(trumpplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Saving the new VADER data into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cretaing csvs from dataframes\n",
    "foxData2.to_csv('ProjectData/foxData2.csv')\n",
    "cnnData2.to_csv('ProjectData/cnnData2.csv')\n",
    "msnbcData2.to_csv('ProjectData/msnbcData2.csv')\n",
    "trumpData2.to_csv('ProjectData/trumpData2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Data Analysis Step 5\n",
    "The analysis step was fairly successful. We were able to conclude that Donald Trump by far had the most positive sentiment in Tweets over the last few months whereas FoxNews had the most negative. CNN and MSNBC were mostly neutral because their compound score was around 0 on a scale from -1 to 1. \n",
    "\n",
    "The next steps in the analysis is to visualize the sentiment of each Twitter user through the Pandemic weeks (End of February through Now) by grouping the tweet sentiments by day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualititative Section Step 6\n",
    "\n",
    "# Step 6 Conclusions and Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the sentiment of tweets varies widely, we can see some important correlation between certain tweets and stock market performance. Most notably, the Dow Jones and S&P 500's fall started to stabilize and rebound around March 20th, when Donald Trump's tweets were particularly positive. On March 19th, he said \"THANK YOU!\" twice, and \"We are going to WIN, sooner rather than later!\" as well as \"America’s Private Sector is stepping up to help us be STRONG!\" However, the indexes took a smaller dip at the end of March, when Fox News wrote an article (which Trump tweeted) about how he said the Coronavirus wouldn't peak until around Easter, and social distancing guidelines would be extended until at least the end of April. Fox also tweeted articles about how travel restrictions had reached unprecidented levels and lab supplies from China were drying up on the same day. Most recently, the stock market took another small around May 3rd, when CNN tweeted an article about how pandemics are going to become more frequent and more deadly in the future. That same day, MSNBC tweeted a news clip criticizing Trump's handling of the pandemic with the words \"To be robbed of a human being who can feel our grief is as disorienting as the lack of explanations around testing\", which recieved a lot of attention (about 10 times as many likes and retweets as their average tweets).\n",
    "\n",
    "There are some fluctuations in the stock market which don't have as strong correlations to gatekeepers' sentiment, or which are the result of tweets on topics other than COVID-19. However, it's evident that the sentiment in tweets does have an effect on the economy. The COVID-19 pandemic has been an uncertain time for everyone, but this study makes it clear that positive language does indeed have a positive impact on the stock market, and negative language has the opposite effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Listing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nikhil Pereira: Formatted the entire notebook to include Phase 1 Content and QQQ Format, Updated API Calls for better data and visualized the Trump Data Recent 20 Posts, Created Time formatting function and applied to Data Frames, Created the Sentiment Bar Plot, Annotated every section to include descriptions and summaries.Created and Presented for Final Presentation.\n",
    "\n",
    "Julia Hoffman: Created Web Scraping Functions \"getTitle\" and \"titleCleaner\", Applied Web Scraping to Data Frames using BeautifulSoup module, Updated Financial API Call using Pandas Data Reader, Updated Financial Plots with Seaborn, Created Box Plot, Wrote a few descriptions. Plotted the Sentiment of Gatekeepers through the timeline. Created and Presented Final Presentation.\n",
    "\n",
    "Ryan Weiss: Created the Vader Score Function and applied it to all the Twitter User Data Frames, wrote problem statement, helped adjust sentiment over time plots, wrote conclusion comparing them to changes in the stock market. Found average sentiment score per gatekeeper per day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
